This guide is still under construction! For sources look into the README file.

**1. The Idea of using an ML/Data Science solution emerges**

*Scenario: The company (or any other organization) you work in wants to introduce "something with AI" into the business, confident that they are going to profit off of it. 
Soon they sink vast amounts of money into new, hyped up technologies - and then comes the big disappointment. 

Apparently, a big stumbling block in the application of ML technologies is that - lost in all the (often justified) excitement - very 
important questions mistakenly are taken for unimportant and redundant. But especially in the beginning the apparently most basic 
questions turn out to be incredibly powerful and defining for the quality of the entire project. 

Here are some super important questions and actionable steps to look at when considering starting an ML project :

* Why is this problem important for our organization?
  * What are the areas of concern that are hoped to be issued?
  * What is the return on investment? 
  * Try to get different perspectives from people who work in different departments of the business in order to get the full picure!

* Who in the organization is affected by this problem? 
  * Who are the people whos work will be affected by this and how? 
  * Talk to them, consider all layers of the organization, bring them together

* Is the data we have useful for solving this problem?
  * If yes, is any more needed in order to get the full picture? Bring people together in order to identify other factors that may impact the problem and get the data. 
  * Can we also derive answers to other questions from it, if so, which - and (why) are they important?
  * If not, what does the appropriate data look like and how can we gather it?

* By what metric do we measure the projects success and when do we consider it to be "over"?
  * Align expectations, what is the final deliverable, and (how) is it useful?

* What if we are not happy about the results?
  * Make sure to have a backup plan figured out to communicate results that may oppose the desired outcome
  * Communicate this possibility and get opinions
  * Stress the importance of reliability and validity of outcomes over "likeable" results

**Further things to consider:**

  * Do not overfocus on either methodology nor final deliverable; make sure to **thoroughly understand the methodology you intend to use, ensure it being chosen and fitted for the problem at hand, and apply it in order to meaningfully answer a question that matters**. 
  * Do not get lost in excitement for new methodology trends and technologies and always put profound understanding and suitability of the method over a mistaken ambition to be "on trend".
  * When a methodology has been chosen, do research on its weaknesses and keep tabs on their effects in different steps of the project.
  * Accept the fact that variation is normal. Consider the difference between measurement variation and random variation in your data. You may run the risk that decisions may be made based on random variation that *cannot be controlled*. Instead, we should make sure to correctly surface actually underlying processes that *we can control.*
  * A relevant source of variation may be how the data is being gathered. For example, scales where customers can rate their satisfaction on a scale from 1 to 10 can create variation as in for one of them a rating of 10 can be the same thing as a rating of 5 for another. One may have not found a product but is happy to have been offered a substitute, another may not have found a product and is *mad* to *only* have been offered a substitute. A rating of this kind may be able to depict satisfaction in a subjective way, but is not a suitable method for f.e. measuring employee performance in a reliable way.
  * Make sure your database is large enough - if not, your model may be very vulnerable to distortion caused by outliers.





